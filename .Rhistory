closed_corpus =  tm_map(closed_corpus, stripWhitespace)
closed_dtm <- DocumentTermMatrix(closed_corpus,control = list(weighting = weightTfIdf))
closed_dtm = removeSparseTerms(closed_dtm, 0.95)
inspect(closed_dtm)
vector.class <- c(rep(T,1000),rep(F,1000))
train <- sample(1:2000,1400)
test <- sample(c(1:2000)[-train],600)
closed.training <- as.matrix(closed_dtm)[train,]
closed.testing <- as.matrix(closed_dtm)[test,]
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 5)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, degree = 5)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "radial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 5)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
library("C50", lib.loc="~/R/win-library/3.3")
tree.model <- C5.0(x= closed.training, y = as.factor(vector.class[train]))
p <- predict(tree.model,closed.testing)
table(p ,as.factor(vector.class[test]))
library("e1071", lib.loc="~/R/win-library/3.3")
vector.class.2 <- c(rep("T",1000),rep("F",1000))
closed.test <- cbind(closed.testing,vector.class.2[test])
closed.train <- cbind(closed.training, vector.class.2[train])
colnames(closed.train)[57] <- "funcion"
colnames(closed.test)[57] <- "funcion"
naive.model <- naiveBayes(V95 ~. , data = closed.train)
naive.model <- naiveBayes(V93 ~. , data = closed.train)
naive.model <- naiveBayes(V92 ~. , data = closed.train)
View(closed.train)
naive.model <- naiveBayes(V94 ~. , data = closed.train)
naive.model <- naiveBayes(x = closed.training , y = vector.class[train] , data = closed.train)
p1 <- predict(naive.model,closed.testing)
table(p1, vector.class[test])
naive.questions <- naiveBayes(x= as.matrix(questions_dtm)[,train], y = questions_mining$score[train])
naive.questions <- naiveBayes(x= as.matrix(questions_dtm)[train,], y = questions_mining$score[train])
pp <- predict(naive.questions,question.testing)
table(pp, questions_mining$score[test])
load("C:/Users/alfredo/Documents/projects/Ciencia de Datos/stackr/questions_2015.Rdata")
questions_corpus <- Corpus(VectorSource(questions$body))
questions_corpus = tm_map(questions_corpus, removePunctuation)
questions_corpus = tm_map(questions_corpus, removeNumbers)
questions_corpus = tm_map(questions_corpus, removeWords, c( stopwords("english")))
questions_corpus =  tm_map(questions_corpus, stripWhitespace)
questions_dtm <- DocumentTermMatrix(questions_corpus,control = list(weighting = weightTfIdf))
questions_dtm = removeSparseTerms(questions_dtm, 0.95)
inspect(questions_dtm)
for(i in 1:nrow(questions)){
if(questions[i,6] < 0){
questions[i,6] = -1
}
if(questions[i,6] > 0){
questions[i,6] = 1
}
}
questions_mining <- cbind(questions$score,as.matrix(questions_dtm))
questions_mining <- as.data.frame(questions_mining)
colnames(questions_mining)[1] <- "score"
train <- sample(1:10000,7000)
test <- sample(c(1:10000)[-train],3000)
questions.training <- questions_mining[train,]
question.testing <- questions_mining[test,]
naive.questions <- naiveBayes(x= as.matrix(questions_dtm)[train,], y = questions_mining$score[train])
pp <- predict(naive.questions,question.testing)
table(pp, questions_mining$score[test])
length(pp)
questions_mining$score <- as.factor(questions_mining$score)
train <- sample(1:10000,7000)
test <- sample(c(1:10000)[-train],3000)
questions.training <- questions_mining[train,]
question.testing <- questions_mining[test,]
naive.questions <- naiveBayes(x= as.matrix(questions_dtm)[train,], y = questions_mining$score[train])
pp <- predict(naive.questions,question.testing)
table(pp, questions_mining$score[test])
plot(naive.questions)
library("ggplot2", lib.loc="~/R/win-library/3.3")
ggplot(naive.questions)
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 0.1)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 5)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = -5)
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 20)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 50)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 200)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 1000)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "radial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 10000)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "polynomial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 1000, degree = 10)
pred.svm = predict(model.svm, question.testing)
table(question.testing$score,pred.svm,dnn=c("Obs","Pred"))
model.svm <- svm(questions.training$score ~. ,data = questions.training,kernel = "polynomial",class.weights = c("-1" = 0.0235,"0" = 0.2018,"1" = 0.7747), cost = 1000, degree = 30)
library("arules", lib.loc="~/R/win-library/3.3")
closed.questions <- stack_search("probability", closed = T, pagesize=100,num_pages=10,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
library(stackr)
library(tm)
library(stringr)
library(dplyr)
library(wordcloud2)
library(e1071)
closed.questions <- stack_search("probability", closed = T, pagesize=100,num_pages=10,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
noclosed.questions <-  stack_search("probability", closed = F, pagesize=100,num_pages=10,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
all.questions <- c(closed.questions$body,noclosed.questions$body)
closed_corpus <- Corpus(VectorSource(all.questions))
closed_corpus = tm_map(closed_corpus, removeNumbers)
closed_corpus = tm_map(closed_corpus, removePunctuation)
closed_corpus = tm_map(closed_corpus, removeWords, c(stopwords("english")))
closed_corpus =  tm_map(closed_corpus, stripWhitespace)
closed_dtm <- DocumentTermMatrix(closed_corpus,control = list(weighting = weightTfIdf))
closed_dtm = removeSparseTerms(closed_dtm, 0.95)
inspect(closed_dtm)
vector.class <- c(rep(T,1000),rep(F,1000))
train <- sample(1:2000,1400)
test <- sample(c(1:2000)[-train],600)
closed.training <- as.matrix(closed_dtm)[train,]
closed.testing <- as.matrix(closed_dtm)[test,]
closed.training <- as.matrix(closed_dtm)[train,]
1189*70/100
1189-832
vector.class <- c(rep(T,189),rep(F,1000))
train <- sample(1:1189,832)
test <- sample(c(1:1189)[-train],357)
closed.training <- as.matrix(closed_dtm)[train,]
closed.testing <- as.matrix(closed_dtm)[test,]
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 5)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
268+13
281*100/357
library("C50", lib.loc="~/R/win-library/3.3")
tree.model <- C5.0(x= closed.training, y = as.factor(vector.class[train]))
p <- predict(tree.model,closed.testing)
table(p ,as.factor(vector.class[test]))
vector.class.2 <- c(rep("T",189),rep("F",1000))
closed.test <- cbind(closed.testing,vector.class.2[test])
closed.train <- cbind(closed.training, vector.class.2[train])
naive.model <- naiveBayes(x = closed.training , y = vector.class[train] , data = closed.train)
p1 <- predict(naive.model,closed.testing)
table(p1, vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 5, cost = 500)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 5, cost = 5000)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 10)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 20)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.9, degree = 20)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 1)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 7)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
289*100/357
model.svm <- svm(closed.training, y= vector.class[train], kernel = "linear",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "radial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "radial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, cost = 1000)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
model.svm <- svm(closed.training, y= vector.class[train], kernel = "radial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, gamma = 10)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
table(p ,as.factor(vector.class[test]))
tree.model <- C5.0(x= closed.training, y = as.factor(vector.class[train]), trials = 5)
p <- predict(tree.model,closed.testing)
table(p ,as.factor(vector.class[test]))
tree.model <- C5.0(x= closed.training, y = as.factor(vector.class[train]), trials = 10)
p <- predict(tree.model,closed.testing)
table(p ,as.factor(vector.class[test]))
tree.model <- C5.0(x= closed.training, y = as.factor(vector.class[train]), trials = 5)
p <- predict(tree.model,closed.testing)
table(p ,as.factor(vector.class[test]))
closed.questions <- stack_search("algebra", closed = T, pagesize=100,num_pages=10,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
noclosed.questions <-  stack_search("algebra", closed = F, pagesize=100,num_pages=10,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
all.questions <- c(closed.questions$body,noclosed.questions$body)
length(all.questions)
closed_corpus <- Corpus(VectorSource(all.questions))
closed_corpus = tm_map(closed_corpus, removeNumbers)
closed_corpus = tm_map(closed_corpus, removePunctuation)
closed_corpus = tm_map(closed_corpus, removeWords, c(stopwords("english")))
closed_corpus =  tm_map(closed_corpus, stripWhitespace)
closed_dtm <- DocumentTermMatrix(closed_corpus,control = list(weighting = weightTfIdf))
closed_dtm = removeSparseTerms(closed_dtm, 0.95)
inspect(closed_dtm)
vector.class <- c(rep(T,113),rep(F,1000))
113*0.7
1113*0.7
train <- sample(1:1113,780)
test <- sample(c(1:1113)[-train],233)
closed.training <- as.matrix(closed_dtm)[train,]
closed.testing <- as.matrix(closed_dtm)[test,]
model.svm <- svm(closed.training, y= vector.class[train], kernel = "polynomial",type= "one-classification", cross = 10, cachesize = 200, nu= 0.4, degree = 7)
pred.svm = predict(model.svm, closed.testing)
table(pred.svm,vector.class[test])
205*100/233
questions_2000 <- stack_search("real analysis",pagesize=100,num_pages=20,fromdate=1420070400,todate=1451606400,filter = "!9YdnSIN18")
questions_2000 <- stack_search("real analysis",pagesize=100,num_pages=20,filter = "!9YdnSIN18")
questions_2000 <- stack_search("probability",pagesize=100,num_pages=20,filter = "!9YdnSIN18")
questions_corpus <- Corpus(VectorSource(questions_2000$body))
questions_corpus = tm_map(questions_corpus, content_transformer(tolower))
questions_corpus = tm_map(questions_corpus, removeNumbers)
questions_corpus = tm_map(questions_corpus, removePunctuation)
questions_corpus = tm_map(questions_corpus, removeWords, c(stopwords("english")))
questions_corpus =  tm_map(questions_corpus, stripWhitespace)
questions_dtm <- DocumentTermMatrix(questions_corpus,control = list(weighting = weightTfIdf))
questions_dtm = removeSparseTerms(questions_dtm, 0.95)
inspect(questions_dtm)
questions_mining <- cbind(questions_2000$score,as.matrix(questions_dtm))
questions_mining <- as.data.frame(questions_mining)
colnames(questions_mining)[1] <- "score"
colnames(questions_mining)[2:114] <- paste0("c",2:134)
colnames(questions_mining)[2:134] <- paste0("c",2:134)
train <- sample(1:2000,1400)
test <- sample(c(1:2000)[-train],600)
questions.training <- questions_mining[train,]
question.testing <- questions_mining[test,]
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116 , data = questions.training)
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 3){count = count + 1}else{count = count}
}
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 10)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 4)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.25)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.1)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.001)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
423*100/600
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.000000001)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.01,epsilon = 1)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.01,epsilon = 0.01)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
model.svr <- svm( score ~ c2+c3+c4+c5+c6+c7+c8+c9+c10+c11+c12+c13+c14+c15+c16+c17+c18+c19+c20+c21+c22+c23+c24+c25+c26+c27+c28+c29+c30+c31+c32+c33+c34+c35+c36+c37+c38+c39+c40+c41+c42+c43+c44+c45+c46+c47+c48+c49+c50+c51+c52+c53+c54+c55+c56+c57+c58+c59+c60+c61+c62+c63+c64+c65+c66+c67+c68+c69+c70+c71+c72+c73+c74+c75+c76+c77+c78+c79+c80+c81+c82+c83+c84+c85+c86+c87+c88+c89+c90+c91+c92+c93+c94+c95+c96+c97+c98+c99+c100+c101+c102+c103+c104+c105+c106+c107+c108+c109+c110+c111+c112+c113+c114+c115+c116+c117+c118+c119+c120+c121+c122+c123+c124+c125+c126+c127+c128+c129+c130+c131+c132+c133+c134 , data = questions.training, cost = 0.01,cross = 10)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
fmla <- as.formula(paste("score ~ ", paste(colnames(questions.training)[2:134], collapse= "+")))
model.svr <- svm( fmla , data = questions.training, cost = 0.01,cross = 10)
pred.svm = predict(model.svr, question.testing)
error <- question.testing$score - pred.svm
count = 0
for(i in 1:600){
if (abs(error[i]) <= 1){count = count + 1}else{count = count}
}
setwd("~/projects/Ciencia de Datos/Final-Project")
library(knitr)
kable(questions_mining[1:5,1:8])
install.packages("prettydoc")
library("prettydoc", lib.loc="~/R/win-library/3.3")
getwd()
setwd("~/projects/Ciencia de Datos/Final-Project/Final-Project")
stack_tags()
library("stackr", lib.loc="~/R/win-library/3.3")
stack_tags()
library(ggplot2)
tag_corpus <- Corpus(VectorSource(questions$tags))
library(tm)
library(stringr)
library(wordcloud2)
tag_corpus <- Corpus(VectorSource(questions$tags))
tag_corpus = tm_map(tag_corpus, removeNumbers)
tag_corpus = tm_map(tag_corpus, removePunctuation)
tag_corpus = tm_map(tag_corpus, removeWords, c("then","hence","cdot","cdots","number","mathbb","mathbbr","blockquote","doesnt","therefore","for","forall","int","like","such","let","be","the", "and", stopwords("english")))
tag_corpus =  tm_map(tag_corpus, stripWhitespace)
tag_dtm <- DocumentTermMatrix(tag_corpus)
inspect(tag_dtm)
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
freq_terms <- findFreqTerms(tag_dtm,lowfreq = 100)
f <- c()
for (t in freq_terms){
f <- c(f,colSums(as.matrix(tag.dataframe[ ,t])))
}
freq.terms.dataframe <- data.frame(freq_terms,f)
ggplot(freq.terms.dataframe, aes(x = freq_terms,y = f)) + geom_bar(stat = "identity",  width = 0.5) + coord_flip() + xlab("Tags") + ylab("Frecuency")
freq_terms <- findFreqTerms(tag_dtm,lowfreq = 50)
f <- c()
for (t in freq_terms){
f <- c(f,colSums(as.matrix(tag.dataframe[ ,t])))
}
freq.terms.dataframe <- data.frame(freq_terms,f)
ggplot(freq.terms.dataframe, aes(x = freq_terms,y = f)) + geom_bar(stat = "identity",  width = 0.5) + coord_flip() + xlab("Tags") + ylab("Frecuency")
tag_dtm = removeSparseTerms(tag_dtm, 0.95)
inspect(tag_dtm)
tag_dtm <- DocumentTermMatrix(tag_corpus)
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
wordcloud2(tag.dataframe)
wordcloud2(tag.dataframe, size = 0.1)
tag_dtm = removeSparseTerms(tag_dtm, 0.5)
inspect(tag_dtm)
stack_tags()
tags <- stack_tags()   # Estos son los que faltan descargar
tags <- stack_tags(pagesize = 100, num_pages = 5)   # Estos son los que faltan descargar
questions_2000 <- stack_search("probability",pagesize=100,num_pages=20,filter = "!9YdnSIN18")
save(questions_2000, file = "questions 2000.Rdata")
save(tags, file = "tags.Rdata")
getwd()
View(tags)
load(file = "tags.Rdata")
df <- select(tags, name, count)
library(ggplot2)
library(dplyr)
df <- select(tags, name, count)
nube <- wordcloud2(df, size = 0.5)
nube
nube <- wordcloud2(df, size = 0.5, ellipticity = 180, backgroundColor = "black")
nube
nube <- wordcloud2(df, size = 0.5, ellipticity = 250, shape = "circle")
nube
nube <- wordcloud2(df, size = 1, shape = "circle")
nube
as.data.frame(table(questions$is_answered))
ggplot(questions,aes(x = is_answered)) + geom_histogram(stat = "count", width = 0.5) + xlab("Is answered") + ylab("Frequency")
no_answered <- filter(questions_2000, questions_2000$is_answered == F)
tag_noa_corpus <- Corpus(VectorSource(no_answered$tags))
tag_noa_corpus = tm_map(tag_noa_corpus, removeNumbers)
tag_noa_corpus = tm_map(tag_noa_corpus, removePunctuation)
tag_noa_corpus = tm_map(tag_noa_corpus, removeWords, c("then","hence","cdot","cdots","number","mathbb","mathbbr","blockquote","doesnt","therefore","for","forall","int","like","such","let","be","the", "and", stopwords("english")))
tag_noa_corpus =  tm_map(tag_noa_corpus, stripWhitespace)
tag_noa_dtm <- DocumentTermMatrix(tag_noa_corpus)
inspect(tag_noa_dtm)
tag.noa.dataframe <- as.data.frame(as.matrix(tag_noa_dtm))
freq_noa_terms <- findFreqTerms(tag_noa_dtm,lowfreq = 5)
f_noa <- c()
for (t in freq_noa_terms){
f_noa <- c(f_noa,colSums(as.matrix(tag.noa.dataframe[ ,t])))
}
freq.terms.noadataframe <- data.frame(freq_noa_terms,f_noa)
ggplot(freq.terms.noadataframe, aes(x = freq_noa_terms,y = f_noa)) + geom_bar(stat = "identity",  width = 0.3) + coord_flip() + xlab("Tags") + ylab("Frecuency")
no_answered <- filter(questions, questions$is_answered == F)
tag_noa_corpus <- Corpus(VectorSource(no_answered$tags))
tag_noa_corpus = tm_map(tag_noa_corpus, removeNumbers)
tag_noa_corpus = tm_map(tag_noa_corpus, removePunctuation)
tag_noa_corpus = tm_map(tag_noa_corpus, removeWords, c("then","hence","cdot","cdots","number","mathbb","mathbbr","blockquote","doesnt","therefore","for","forall","int","like","such","let","be","the", "and", stopwords("english")))
tag_noa_corpus =  tm_map(tag_noa_corpus, stripWhitespace)
tag_noa_dtm <- DocumentTermMatrix(tag_noa_corpus)
inspect(tag_noa_dtm)
tag.noa.dataframe <- as.data.frame(as.matrix(tag_noa_dtm))
freq_noa_terms <- findFreqTerms(tag_noa_dtm,lowfreq = 5)
f_noa <- c()
for (t in freq_noa_terms){
f_noa <- c(f_noa,colSums(as.matrix(tag.noa.dataframe[ ,t])))
}
freq.terms.noadataframe <- data.frame(freq_noa_terms,f_noa)
ggplot(freq.terms.noadataframe, aes(x = freq_noa_terms,y = f_noa)) + geom_bar(stat = "identity",  width = 0.3) + coord_flip() + xlab("Tags") + ylab("Frecuency")
freq_noa_terms <- findFreqTerms(tag_noa_dtm,lowfreq = 10)
f_noa <- c()
for (t in freq_noa_terms){
f_noa <- c(f_noa,colSums(as.matrix(tag.noa.dataframe[ ,t])))
}
freq.terms.noadataframe <- data.frame(freq_noa_terms,f_noa)
ggplot(freq.terms.noadataframe, aes(x = freq_noa_terms,y = f_noa)) + geom_bar(stat = "identity",  width = 0.3) + coord_flip() + xlab("Tags") + ylab("Frecuency")
ggplot(df, aes(x = name, y = count)) + geom_bar(stat = "identity", width = 1)
ggplot(df, aes(x = name, y = count)) + geom_bar(stat = "identity", width = 1) + coord_flip()
ggplot(df[1:20, ], aes(x = name, y = count)) + geom_bar(stat = "identity", width = 1) + coord_flip()
ggplot(df[1:20, ], aes(x = name, y = count)) + geom_bar(stat = "identity", width = 0.5) + coord_flip()
grafico.1 <- ggplot(df[1:20, ], aes(x = name, y = count)) + geom_bar(stat = "identity", width = 0.5) + coord_flip()
library(tm)
library(stringr)
library(dplyr)
library(e1071)
load(file = "informacion.Rdata")
load( file = "questions_2015.Rdata")
questions_corpus <- Corpus(VectorSource(questions$body))
questions_corpus = tm_map(questions_corpus, removeNumbers)
questions_corpus = tm_map(questions_corpus, removePunctuation)
questions_corpus = tm_map(questions_corpus, removeWords, c( stopwords("english")))
questions_corpus =  tm_map(questions_corpus, stripWhitespace)
questions_dtm <- DocumentTermMatrix(questions_corpus,control = list(weighting = weightTfIdf))
questions_dtm = removeSparseTerms(questions_dtm, 0.95)
inspect(questions_dtm)
for(i in 1:nrow(questions)){
if(questions[i,6] < 0){
questions[i,6] = -1
}
if(questions[i,6] > 0){
questions[i,6] = 1
}
}
questions_mining <- cbind(questions$score,as.matrix(questions_dtm))
questions_mining <- as.data.frame(questions_mining)
colnames(questions_mining)[1] <- "score"
questions_mining$score <- as.factor(questions_mining$score)
train <- sample(1:10000,7000)
test <- sample(c(1:10000)[-train],3000)
questions.training <- questions_mining[train,]
question.testing <- questions_mining[test,]
naive.questions <- naiveBayes(x= as.matrix(questions_dtm)[train,], y = questions_mining$score[train])
pp <- predict(naive.questions,question.testing)
table(pp, questions_mining$score[test])
knitr::opts_chunk$set(echo = TRUE)
source("exploratory_analysis.R")
grafico.2 <- ggplot(freq.terms.dataframe, aes(x = freq_terms,y = f)) + geom_bar(stat = "identity",  width = 0.5) + coord_flip() + xlab("Tags") + ylab("Frecuency")
grafico.3 <- ggplot(questions,aes(x = is_answered)) + geom_histogram(stat = "count", width = 0.5) + xlab("Is answered") + ylab("Frequency")
grafico.4 <- ggplot(freq.terms.noadataframe, aes(x = freq_noa_terms,y = f_noa)) + geom_bar(stat = "identity",  width = 0.3) + coord_flip() + xlab("Tags") + ylab("Frecuency")
source("score_classification.R")
source("exploratory_analysis.R")
source("exploratory_analysis.R")
source("exploratory_analysis.R")
source("score_classification.R")
library(ggplot2)
library(tm)
library(stringr)
library(wordcloud2)
library(dplyr)
load(file = "tags.Rdata")
df <- select(tags, name, count)
nube <- wordcloud2(df, size = 0.8, shape = "circle")
grafico.1 <- ggplot(df[1:20, ], aes(x = name, y = count)) + geom_bar(stat = "identity", width = 0.5) + coord_flip()
tag_corpus <- Corpus(VectorSource(questions$tags))
tag_corpus = tm_map(tag_corpus, removeNumbers)
tag_corpus = tm_map(tag_corpus, removePunctuation)
tag_corpus = tm_map(tag_corpus, removeWords, c("then","hence","cdot","cdots","number","mathbb","mathbbr","blockquote","doesnt","therefore","for","forall","int","like","such","let","be","the", "and", stopwords("english")))
tag_corpus =  tm_map(tag_corpus, stripWhitespace)
tag_dtm <- DocumentTermMatrix(tag_corpus)
inspect(tag_dtm)
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
tag.dataframe <- as.data.frame(as.matrix(tag_dtm))
tag.dataframe <- as.matrix(tag_dtm)
tag_dtm <- removeSparseTerms(tag_dtm, 0.3)
inspect(tag_dtm)
tag_dtm <- removeSparseTerms(tag_dtm, 100)
tag_dtm <- removeSparseTerms(tag_dtm, 1)
tag_dtm <- removeSparseTerms(tag_dtm, 0.99)
inspect(tag_dtm)
tag_dtm <- removeSparseTerms(tag_dtm, 0)
tag_dtm <- DocumentTermMatrix(tag_corpus)
tag.dataframe <- as.matrix(tag_dtm)
